{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P17gLUnRWjMX"
   },
   "source": [
    "# Recurrent neural network model for text Generation\n",
    "### Local imports & execution\n",
    "\n",
    "## Libraries & imports\n",
    "Some assets to work on the dataset and train a Deep learning model.\n",
    "We might need tools for:\n",
    "\n",
    "* system operations\n",
    "* string management\n",
    "* plotting\n",
    "\n",
    "And off course, getting involved with Deep Learning Models. For this purpose I will be using Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJumAjwFV044",
    "outputId": "a957645e-7c1f-4423-ca01-1ac1f2d4d3ab"
   },
   "outputs": [],
   "source": [
    "#Utils\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "#Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout, Flatten\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional,GRU, TimeDistributed\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "#sys\n",
    "import sys\n",
    "import io\n",
    "\n",
    "\n",
    "drivepath= \"data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Pvliy44j5P4K"
   },
   "outputs": [],
   "source": [
    "# Run this to ensure TensorFlow 2.x is used\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XONL-0jPyhIZ"
   },
   "source": [
    "# TEXT MANAGEMENT\n",
    "\n",
    "##Text import and consolidation\n",
    "\n",
    "three texts are imported to build the model:\n",
    "\n",
    "* Game of Thrones\n",
    "* The Bible\n",
    "* The lord of the rings\n",
    "\n",
    "They may be similar in content, after all, all three of them use quite often the word *lord*.\n",
    "\n",
    "For now, the script will load the text sequences, process them into a *clean* non-punctuated text string, and finally tokkenize the text into words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvc5vB_bzDWF",
    "outputId": "50572379-1f23-4d02-e6a5-adc61c1d619f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text length : 1343116\n",
      " \n",
      " Trump: Wow. Whoa. That is some group of people. Thousands. So nice, thank you very much. That's really nice. Thank you. It's great to be at Trump Tower. It's great to be in a wonderful city, New Yo\n",
      "--------------------\n",
      "Tokkens length : 230321\n",
      "['wow', 'whoa', 'that', 'is', 'some']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fucntion to retrieve and consolidste txt\n",
    "def build_raw_text(origins,maxlen):\n",
    "    ans  = \"\"  #answer string\n",
    "\n",
    "    each_part = int(maxlen/len(origins)) #how much charaxters of each text \n",
    "\n",
    "    for elem in origins: #for every text\n",
    "        try:\n",
    "            with open(drivepath+\"text_origin_{lib}.txt\".format(lib=elem),\"r\") as fp: #open the text\n",
    "                lines = ''.join(fp.readlines()) #read lines\n",
    "                ans = ans +\" \\n \"+ lines[:each_part] #add the lines \n",
    "        except e: #if error\n",
    "            print(e)\n",
    "    return ans\n",
    "\n",
    "# function to correct text and okenize into words\n",
    "def clean_text(doc,filter_words=[]):\n",
    "    tokens = doc.split() #split the text in words\n",
    "    table = str.maketrans('', '', string.punctuation) #replace punctuation\n",
    "    tokens = [w.translate(table) for w in tokens] \n",
    "    tokens = [word for word in tokens if word.isalpha()] # only keep alphanumeric tokkens\n",
    "    tokens = [word.lower() for word in tokens] #lowercase\n",
    "    if(len(filter_words)!=0):\n",
    "        tokens = list(filter(lambda k: k not in filter_words, tokens))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "#ors = [\"bible\",\"got\",\"lotr\"]\n",
    "#ors = [\"poemas\"]\n",
    "ors = [\"trump\"]\n",
    "\n",
    "maxlim_chars = 5000000 #max limit of characters\n",
    "raw_text = build_raw_text(ors,maxlim_chars)\n",
    "token_text = clean_text(raw_text,filter_words=[\"trump\"])\n",
    "\n",
    "print(\"Raw text length :\",len(raw_text))\n",
    "print(raw_text[:200])\n",
    "print(\"-\"*20)\n",
    "print(\"Tokkens length :\",len(token_text))\n",
    "print(token_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1uldFM50r3V"
   },
   "source": [
    "As we can see, the text modeling went well. We can count the number of characters of the raw text imported and the number of tokkens this operation finally got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zA4rM_dl0-mC"
   },
   "source": [
    "## Token package creation\n",
    "\n",
    "In order to train our model we might first create a set of inputs-outputs for our model. The final model will work like this: we input a set of $N$ **ordered** words, and we expect as aon output the **next single word** following this sequence. A suitable training set for this purpose would be different sets of $N+1$ words, in which we take out the las word of the sequence and feed the model with the purpose of predicting this last word given the other $N$ words. I call these corresponding sets of $N+1$ words a **token package** and the following part of the code will be focused on creating them.\n",
    "\n",
    "**NOTE:** Token is just another fancy word for *word*. We use token to describe a compact object or entity in the context of text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eW_sjMS5238U",
    "outputId": "7de1f584-d9f9-43f8-e7bd-eb7699d86ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token poackages  : 199951\n",
      "wow whoa that is some group of people thousands so nice thank you very much thats really nice thank you its great to be at tower its great to be in a wonderful city new york and its an honor to have everybody here this is beyond anybodys expectations theres been\n"
     ]
    }
   ],
   "source": [
    "N = 50 + 1 # length of token package\n",
    "max_count = 200000 #max amount of token packages\n",
    "lines = [] #oputput list with lines\n",
    "\n",
    "for i in range(N, len(token_text)): # we start the counter in N+1\n",
    "    seq = token_text[i-N:i] # we select the N+1 tokens previous to the counter\n",
    "    line = ' '.join(seq) # create the line by joining the words\n",
    "    lines.append(line) # append the line to the list of lines \n",
    "    if i > max_count: \n",
    "        break\n",
    "\n",
    "print(\"Number of token poackages  :\",len(lines))\n",
    "print(lines[0]) #print the first token package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvmJzMsb5wB5"
   },
   "source": [
    "Now we have compact packages of words that may serve for training a learning model. In order for a model to read and process the words in the text, we need to translate them into a type that a numeric model would understand. That is why the next step is to create a *dictionary-like* structure that hel up translate each word into a numerical figure (such as a vector or a scalar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uK3Diwv6up4"
   },
   "source": [
    "## Tokeninzer\n",
    "The *Tokenizer* method allows us to create such dictionary that create an asociation between words and numbers. we first *fit* the tokenizer to create a dictionary with a given set of words. We might need as much words as possible since new words ffor the model will be casssifed as unknown (since they dont have an associated number). We fit the dictionary, and with this dictionary we *translate* new texts into numers using these correspondences. \n",
    "\n",
    "The procedure belowe fits the dictionary on the set of words and translate the same set of words using the created dictionary, so we end up with a set of numbers for each token package instead of a set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Okn5FTD8vZxm"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() # creates a tokenizer object\n",
    "tokenizer.fit_on_texts(lines) #fits on the lines weve created\n",
    "sequences = tokenizer.texts_to_sequences(lines) #and tranlate them as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6q1Ul2f9ELV"
   },
   "source": [
    "Now let's take a look into these sequences compared to the lines given before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsOsnFOxla3n",
    "outputId": "3aab0e76-c61f-45f9-837e-6ef0848b5071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow whoa that is some group of people thousands so nice thank you very much thats really nice thank you its great to be at tower its great to be in a wonderful city new york and its an honor to have everybody here this is beyond anybodys expectations theres been\n",
      "[1501, 5067, 10, 14, 163, 641, 4, 22, 259, 28, 534, 121, 12, 70, 126, 102, 208, 534, 121, 12, 29, 54, 3, 21, 46, 2540, 29, 54, 3, 21, 6, 5, 579, 418, 55, 331, 2, 29, 82, 640, 3, 15, 367, 117, 19, 14, 1184, 3297, 5066, 460, 64]\n",
      "The vocabulary size is  7916\n"
     ]
    }
   ],
   "source": [
    "print(lines[0]) #fist line\n",
    "print(sequences[0]) #fisrt line translated\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "print(\"The vocabulary size is \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmECRcu-9WMB"
   },
   "source": [
    "we can see the correspondence between words and number that the Tokenizer created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZAWr3CR9h64"
   },
   "source": [
    "## Training set creation: splitting token packages\n",
    "In order to create a succesful training set we might need and  input (X) and an output (Y) so we can teach examples to the learning model. As we mentioned before, the idea of an $N+1$ word package is to take the frst $N$ words as an input and the last one word as the expected output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAASA0LG9hfO",
    "outputId": "472a96db-1520-4201-edfe-90909e84bd49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the sequence  [1501 5067   10   14  163  641    4   22  259   28  534  121   12   70\n",
      "  126  102  208  534  121   12   29   54    3   21   46 2540   29   54\n",
      "    3   21    6    5  579  418   55  331    2   29   82  640    3   15\n",
      "  367  117   19   14 1184 3297 5066  460   64]\n",
      "The X input is  [1501 5067   10   14  163  641    4   22  259   28  534  121   12   70\n",
      "  126  102  208  534  121   12   29   54    3   21   46 2540   29   54\n",
      "    3   21    6    5  579  418   55  331    2   29   82  640    3   15\n",
      "  367  117   19   14 1184 3297 5066  460]\n",
      "and the output is  64\n",
      "--------------------\n",
      "Sequence length:  50\n"
     ]
    }
   ],
   "source": [
    "sequences = np.array(sequences) # vectorizing the whole package array  \n",
    "X  = sequences[:, :-1] # for each sequence, take every element but the last one\n",
    "Y = sequences[:,-1]  # for each sequence, take the last element\n",
    "print(\"For the sequence \", sequences[0])\n",
    "print(\"The X input is \", X[0])\n",
    "print(\"and the output is \", Y[0] )\n",
    "\n",
    "seq_length = X.shape[1] # training input sequence length \n",
    "print(\"-\"*20)\n",
    "print(\"Sequence length: \", seq_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y01BRF-x_y1P"
   },
   "source": [
    "## Categorical output: binarizing the Y vector\n",
    "A neural network model works by activating neurons given some operations made internally between data and trainable weighted matrices. This implies that the output is also a set of activations (each activation is a scalar between 0 and 1). For adapting this kind of output into our numerical output Y, we need to use a **categorical transfomattion method**. This method transforms the scalars given into a vector of length equal to the vocabulary size. If the Y output is (for example) 27, the Y categorical vector will be a vector of length `vocab_size` in which every position is a 0, except for the 27th position that will be a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8HUaD-ok_yNm"
   },
   "outputs": [],
   "source": [
    "y = to_categorical(Y, num_classes=vocab_size) # creating categorical vectors Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hu1S30NwBXRp"
   },
   "source": [
    "# DEEP LEARNING MODEL\n",
    "Once created the dataset, we might proceed to create the model to be rained and then, train it with the examples we´ve created.\n",
    "\n",
    "## Model architecture\n",
    "\n",
    "A **sequential model** is a valid architecture for this excercise: we proceed to create layers of neurons that are conected with the previous and next layers of neurons. In this model we will expose the core of the text generation algorith: LSTM neurons. These **layers of neurons** are no longer just transmitting informaion forward, but also keeping notion of the order of the inputs (that is why the sequences needed to be ordered) by keeping **hidden states** that can be transmitted within the layer itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLD0MJf8BW67",
    "outputId": "f05ed645-8ee4-4195-d79d-70ad9bea5c2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            395800    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 64)            29440     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7916)              1021164   \n",
      "=================================================================\n",
      "Total params: 1,487,748\n",
      "Trainable params: 1,487,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(lstm_neurons,dense_neurons):\n",
    "\n",
    "    model = Sequential() # sequential model creation\n",
    "\n",
    "    # add the embedding layer to reduce dimensionality in input vectors\n",
    "    model.add(Embedding(vocab_size, N-1, input_length=seq_length)) \n",
    "\n",
    "    # first LSTM layer, as the next layer is also LSTM this must return seuqnces\n",
    "    model.add(LSTM(lstm_neurons, return_sequences=True))\n",
    "\n",
    "    # second LSTM layer\n",
    "    model.add(LSTM(lstm_neurons))\n",
    "\n",
    "    # first dense layer\n",
    "    model.add(Dense(dense_neurons, activation='relu'))\n",
    "\n",
    "    #output layer, must be of the categorical Y length i.e vocab_size\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    #return model\n",
    "    return model\n",
    "\n",
    "model = create_model(64,128) #creating the model\n",
    "model.summary() #check model characteristics and trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9gtqLIjg_xE3"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for compiling the model we use:\n",
    "  * categorical cross entropy for checking the loss\n",
    "  * ADAM as the algorithm for \"surfing\" the error gradient \n",
    "  * Accuracy to measure performance\n",
    "'''\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVkcwTTSzm5_"
   },
   "source": [
    "## Model fitting\n",
    "\n",
    "now, the long wait . . . \n",
    "The training stage is just the model reading the examples given, many times over and over in order to calibrate the weight matrices. Each time the model reads all the examples is an **epoch** and we will use 50 of this loops to train the model. The model will read batches of 256 examples each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size = 256, epochs = 50) #fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eRMOK1UMRIM"
   },
   "source": [
    "# TEXT GENERATION\n",
    "\n",
    "once the model is trained, we can proceed with the main goal of this notebook: The continous generation of text. For this, we need to take into account that the model receives an $N$ word set as a **seed** for generationg the next word. Given this, it is important that this seed is coherent wih the text structure given as a training set, since the model learned not just to recognize words and to predict them, but to verify the order of the seed words in order to give a reasonaable output as an answer. \n",
    "\n",
    "\n",
    "`generate_text_seq()` generates `n_words` number of words after the given `seed_text`. We are going to pre-process the seed_text before predicting. We are going to encode the seed_text using the same encoding used for encoding the training data. Then we are going to convert the seed_text to $N$ words by using `pad_sequences()`. Now we will predict using `model.predict_classes()`. After that we will search the word in tokenizer using the index in `y_predict` (the output vector). Finally we will append the predicted word to seed_text and text and repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYEI3DuK0Byw"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This method receives:\n",
    " * model: the trained model\n",
    " * tokenizer: the fitted dictionary\n",
    " * text_seq_length: the N length of each word sequence\n",
    " * seed_text: the words that will be used as seed for text generation\n",
    " * n_words: the number of word to be generated\n",
    "'''\n",
    "\n",
    "\n",
    "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n",
    "  \n",
    "    text = []\n",
    "\n",
    "    for _ in range(n_words):\n",
    "        # translate the seed text\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #make the sequences of length N (by padding or truncating)\n",
    "        encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre') \n",
    "\n",
    "        # generate the response vector using the trained model\n",
    "        y_predict = model.predict_classes(encoded) \n",
    "\n",
    "        #get the predicted word\n",
    "        predicted_word = '' \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "          #find it in dictionary\n",
    "          if index == y_predict:\n",
    "            predicted_word = word \n",
    "            break\n",
    "\n",
    "    # append the new word to the seed_text for the next word\n",
    "    seed_text = seed_text + ' ' + predicted_word\n",
    "    #append the new word to the list of words\n",
    "    text.append(predicted_word)\n",
    "\n",
    "    #return a jointed list of words\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gESXIAkYPmwf"
   },
   "source": [
    "### Checking results\n",
    "\n",
    "Now, lets generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "eg0JFybr0kNO",
    "outputId": "196222e2-3dea-4e43-d9b2-d987b4c1ecba"
   },
   "outputs": [],
   "source": [
    "n_lines  = len(lines)\n",
    "print(\"There are \",n_lines,\" lines\")\n",
    "seed_text = lines[190000] # use arbitrary line as seed\n",
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xg3MwYCeP61V",
    "outputId": "be73e6b6-a11f-4785-e943-d7175bf80716"
   },
   "outputs": [],
   "source": [
    "#generate text (100 words)\n",
    "text_gen = generate_text_seq(loaded_model, tokenizer, seq_length, seed_text, 100)\n",
    "\n",
    "print(seed_text + ' ' + text_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3NnZXhePGio"
   },
   "source": [
    "# Extra assets\n",
    "\n",
    "This section might be useful to test results or check some settings in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cu59__QtSsJO"
   },
   "source": [
    "## Using TPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUbQQDin5nsj",
    "outputId": "6d10dcef-a7e8-4353-c73b-c34ce9ff3df9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IS TPU GOING TO BE USED?\n",
    "## THIS MIGHT NOT BE AVAILABLE FOR LOCAL EXECUTION (TPU AVILABLE IN GOOGLE COLAB)\n",
    "\n",
    "call_TPU = True\n",
    "if(call_TPU):\n",
    "    %tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2abK7KoWR42O"
   },
   "source": [
    "## Saving model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aENl7NkJNB7y",
    "outputId": "2b0d0a79-8de7-4844-d251-4edfddc61f7e"
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(drivepath+\"model_textgen_v3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(drivepath+\"model_textgen_v3_weights.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6pCLQ3PR-jr"
   },
   "source": [
    "## Loading model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWk0wTUZSZ8v",
    "outputId": "8306de74-96df-4bc5-b565-b8dec4ccfea0"
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open(drivepath+\"model_textgen_v3.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(drivepath+\"model_textgen_v3_weights.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TextGen_V3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
