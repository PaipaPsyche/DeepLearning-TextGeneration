{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P17gLUnRWjMX"
   },
   "source": [
    "# Recurrent neural network model for text Generation\n",
    "### Drive imports & Collab execution\n",
    "\n",
    "## Libraries & imports\n",
    "Some assets to work on the dataset and train a Deep learning model.\n",
    "We might need tools for:\n",
    "\n",
    "* system operations\n",
    "* string management\n",
    "* plotting\n",
    "\n",
    "And off course, getting involved with Deep Learning Models. For this purpose I will be using Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJumAjwFV044",
    "outputId": "a957645e-7c1f-4423-ca01-1ac1f2d4d3ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#Utils\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "#Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout, Flatten\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional,GRU, TimeDistributed\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "#sys\n",
    "import sys\n",
    "import io\n",
    "\n",
    "\n",
    "#drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "drivepath= \"/content/drive/My Drive/TextGen- G Colab/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pvliy44j5P4K"
   },
   "outputs": [],
   "source": [
    "# Run this to ensure TensorFlow 2.x is used\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XONL-0jPyhIZ"
   },
   "source": [
    "# TEXT MANAGEMENT\n",
    "\n",
    "##Text import and consolidation\n",
    "\n",
    "three texts are imported to build the model:\n",
    "\n",
    "* Game of Thrones\n",
    "* The Bible\n",
    "* The lord of the rings\n",
    "\n",
    "They may be similar in content, after all, all three of them use quite often the word *lord*.\n",
    "\n",
    "For now, the script will load the text sequences, process them into a *clean* non-punctuated text string, and finally tokkenize the text into words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvc5vB_bzDWF",
    "outputId": "50572379-1f23-4d02-e6a5-adc61c1d619f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text length : 5000007\n",
      " \n",
      "   In the beginning God created the heaven and the earth.\n",
      "\n",
      "  And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep. And the Spirit of God moved upon the face of the\n",
      "wa\n",
      "--------------------\n",
      "Tokkens length : 888080\n",
      "['in', 'the', 'beginning', 'god', 'created']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fucntion to retrieve and consolidste txt\n",
    "def build_raw_text(origins,maxlen):\n",
    "    ans  = \"\"  #answer string\n",
    "\n",
    "    each_part = int(maxlen/len(origins)) #how much charaxters of each text \n",
    "\n",
    "    for elem in origins: #for every text\n",
    "        try:\n",
    "            with open(drivepath+\"text_origin_{lib}.txt\".format(lib=elem),\"r\") as fp: #open the text\n",
    "                lines = ''.join(fp.readlines()) #read lines\n",
    "                ans = ans +\" \\n \"+ lines[:each_part] #add the lines \n",
    "        except e: #if error\n",
    "            print(e)\n",
    "    return ans\n",
    "\n",
    "# function to correct text and okenize into words\n",
    "def clean_text(doc,filter_words=[]):\n",
    "    tokens = doc.split() #split the text in words\n",
    "    table = str.maketrans('', '', string.punctuation) #replace punctuation\n",
    "    tokens = [w.translate(table) for w in tokens] \n",
    "    tokens = [word for word in tokens if word.isalpha()] # only keep alphanumeric tokkens\n",
    "    tokens = [word.lower() for word in tokens] #lowercase\n",
    "    if(len(filter_words)!=0):\n",
    "        tokens = list(filter(lambda k: k not in filter_words, tokens))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "#ors = [\"bible\",\"got\",\"lotr\"]\n",
    "#ors = [\"poemas\"]\n",
    "ors = [\"trump\"]\n",
    "\n",
    "maxlim_chars = 5000000 #max limit of characters\n",
    "raw_text = build_raw_text(ors,maxlim_chars)\n",
    "token_text = clean_text(raw_text,filter_words=[\"trump\"])\n",
    "\n",
    "print(\"Raw text length :\",len(raw_text))\n",
    "print(raw_text[:200])\n",
    "print(\"-\"*20)\n",
    "print(\"Tokkens length :\",len(token_text))\n",
    "print(token_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1uldFM50r3V"
   },
   "source": [
    "As we can see, the text modeling went well. We can count the number of characters of the raw text imported and the number of tokkens this operation finally got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zA4rM_dl0-mC"
   },
   "source": [
    "## Token package creation\n",
    "\n",
    "In order to train our model we might first create a set of inputs-outputs for our model. The final model will work like this: we input a set of $N$ **ordered** words, and we expect as aon output the **next single word** following this sequence. A suitable training set for this purpose would be different sets of $N+1$ words, in which we take out the las word of the sequence and feed the model with the purpose of predicting this last word given the other $N$ words. I call these corresponding sets of $N+1$ words a **token package** and the following part of the code will be focused on creating them.\n",
    "\n",
    "**NOTE:** Token is just another fancy word for *word*. We use token to describe a compact object or entity in the context of text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eW_sjMS5238U",
    "outputId": "7de1f584-d9f9-43f8-e7bd-eb7699d86ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token poackages  : 199951\n",
      "in the beginning god created the heaven and the earth and the earth was without form and void and darkness was upon the face of the deep and the spirit of god moved upon the face of the waters and god said let there be light and there was light and\n"
     ]
    }
   ],
   "source": [
    "N = 50 + 1 # length of token package\n",
    "max_count = 200000 #max amount of token packages\n",
    "lines = [] #oputput list with lines\n",
    "\n",
    "for i in range(N, len(token_text)): # we start the counter in N+1\n",
    "    seq = token_text[i-N:i] # we select the N+1 tokens previous to the counter\n",
    "    line = ' '.join(seq) # create the line by joining the words\n",
    "    lines.append(line) # append the line to the list of lines \n",
    "    if i > max_count: \n",
    "        break\n",
    "\n",
    "print(\"Number of token poackages  :\",len(lines))\n",
    "print(lines[0]) #print the first token package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvmJzMsb5wB5"
   },
   "source": [
    "Now we have compact packages of words that may serve for training a learning model. In order for a model to read and process the words in the text, we need to translate them into a type that a numeric model would understand. That is why the next step is to create a *dictionary-like* structure that hel up translate each word into a numerical figure (such as a vector or a scalar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uK3Diwv6up4"
   },
   "source": [
    "## Tokeninzer\n",
    "The *Tokenizer* method allows us to create such dictionary that create an asociation between words and numbers. we first *fit* the tokenizer to create a dictionary with a given set of words. We might need as much words as possible since new words ffor the model will be casssifed as unknown (since they dont have an associated number). We fit the dictionary, and with this dictionary we *translate* new texts into numers using these correspondences. \n",
    "\n",
    "The procedure belowe fits the dictionary on the set of words and translate the same set of words using the created dictionary, so we end up with a set of numbers for each token package instead of a set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Okn5FTD8vZxm"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() # creates a tokenizer object\n",
    "tokenizer.fit_on_texts(lines) #fits on the lines weve created\n",
    "sequences = tokenizer.texts_to_sequences(lines) #and tranlate them as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6q1Ul2f9ELV"
   },
   "source": [
    "Now let's take a look into these sequences compared to the lines given before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsOsnFOxla3n",
    "outputId": "3aab0e76-c61f-45f9-837e-6ef0848b5071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the beginning god created the heaven and the earth and the earth was without form and void and darkness was upon the face of the deep and the spirit of god moved upon the face of the waters and god said let there be light and there was light and\n",
      "[7, 1, 1212, 32, 1211, 1, 260, 2, 1, 134, 2, 1, 134, 31, 240, 5627, 2, 1937, 2, 946, 31, 37, 1, 214, 3, 1, 1755, 2, 1, 572, 3, 32, 1602, 37, 1, 214, 3, 1, 289, 2, 32, 26, 96, 63, 15, 597, 2, 63, 31, 597, 2]\n",
      "The vocabulary size is  5628\n"
     ]
    }
   ],
   "source": [
    "print(lines[0]) #fist line\n",
    "print(sequences[0]) #fisrt line translated\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "print(\"The vocabulary size is \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmECRcu-9WMB"
   },
   "source": [
    "we can see the correspondence between words and number that the Tokenizer created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZAWr3CR9h64"
   },
   "source": [
    "## Training set creation: splitting token packages\n",
    "In order to create a succesful training set we might need and  input (X) and an output (Y) so we can teach examples to the learning model. As we mentioned before, the idea of an $N+1$ word package is to take the frst $N$ words as an input and the last one word as the expected output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAASA0LG9hfO",
    "outputId": "472a96db-1520-4201-edfe-90909e84bd49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the sequence  [   7    1 1212   32 1211    1  260    2    1  134    2    1  134   31\n",
      "  240 5627    2 1937    2  946   31   37    1  214    3    1 1755    2\n",
      "    1  572    3   32 1602   37    1  214    3    1  289    2   32   26\n",
      "   96   63   15  597    2   63   31  597    2]\n",
      "The X input is  [   7    1 1212   32 1211    1  260    2    1  134    2    1  134   31\n",
      "  240 5627    2 1937    2  946   31   37    1  214    3    1 1755    2\n",
      "    1  572    3   32 1602   37    1  214    3    1  289    2   32   26\n",
      "   96   63   15  597    2   63   31  597]\n",
      "and the output is  2\n",
      "--------------------\n",
      "Sequence length:  50\n"
     ]
    }
   ],
   "source": [
    "sequences = np.array(sequences) # vectorizing the whole package array  \n",
    "X  = sequences[:, :-1] # for each sequence, take every element but the last one\n",
    "Y = sequences[:,-1]  # for each sequence, take the last element\n",
    "print(\"For the sequence \", sequences[0])\n",
    "print(\"The X input is \", X[0])\n",
    "print(\"and the output is \", Y[0] )\n",
    "\n",
    "seq_length = X.shape[1] # training input sequence length \n",
    "print(\"-\"*20)\n",
    "print(\"Sequence length: \", seq_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y01BRF-x_y1P"
   },
   "source": [
    "## Categorical output: binarizing the Y vector\n",
    "A neural network model works by activating neurons given some operations made internally between data and trainable weighted matrices. This implies that the output is also a set of activations (each activation is a scalar between 0 and 1). For adapting this kind of output into our numerical output Y, we need to use a **categorical transfomattion method**. This method transforms the scalars given into a vector of length equal to the vocabulary size. If the Y output is (for example) 27, the Y categorical vector will be a vector of length `vocab_size` in which every position is a 0, except for the 27th position that will be a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HUaD-ok_yNm"
   },
   "outputs": [],
   "source": [
    "y = to_categorical(Y, num_classes=vocab_size) # creating categorical vectors Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hu1S30NwBXRp"
   },
   "source": [
    "# DEEP LEARNING MODEL\n",
    "Once created the dataset, we might proceed to create the model to be rained and then, train it with the examples we´ve created.\n",
    "\n",
    "## Model architecture\n",
    "\n",
    "A **sequential model** is a valid architecture for this excercise: we proceed to create layers of neurons that are conected with the previous and next layers of neurons. In this model we will expose the core of the text generation algorith: LSTM neurons. These **layers of neurons** are no longer just transmitting informaion forward, but also keeping notion of the order of the inputs (that is why the sequences needed to be ordered) by keeping **hidden states** that can be transmitted within the layer itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLD0MJf8BW67",
    "outputId": "f05ed645-8ee4-4195-d79d-70ad9bea5c2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            281400    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 64)            29440     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5628)              726012    \n",
      "=================================================================\n",
      "Total params: 1,078,196\n",
      "Trainable params: 1,078,196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(lstm_neurons,dense_neurons):\n",
    "\n",
    "    model = Sequential() # sequential model creation\n",
    "\n",
    "    # add the embedding layer to reduce dimensionality in input vectors\n",
    "    model.add(Embedding(vocab_size, N-1, input_length=seq_length)) \n",
    "\n",
    "    # first LSTM layer, as the next layer is also LSTM this must return seuqnces\n",
    "    model.add(LSTM(lstm_neurons, return_sequences=True))\n",
    "\n",
    "    # second LSTM layer\n",
    "    model.add(LSTM(lstm_neurons))\n",
    "\n",
    "    # first dense layer\n",
    "    model.add(Dense(dense_neurons, activation='relu'))\n",
    "\n",
    "    #output layer, must be of the categorical Y length i.e vocab_size\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    #return model\n",
    "    return model\n",
    "\n",
    "model = create_model(64,128) #creating the model\n",
    "model.summary() #check model characteristics and trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gtqLIjg_xE3"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for compiling the model we use:\n",
    "  * categorical cross entropy for checking the loss\n",
    "  * ADAM as the algorithm for \"surfing\" the error gradient \n",
    "  * Accuracy to measure performance\n",
    "'''\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVkcwTTSzm5_"
   },
   "source": [
    "## Model fitting\n",
    "\n",
    "now, the long wait . . . \n",
    "The training stage is just the model reading the examples given, many times over and over in order to calibrate the weight matrices. Each time the model reads all the examples is an **epoch** and we will use 50 of this loops to train the model. The model will read batches of 256 examples each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmIXPIZnI-v2",
    "outputId": "7c03171b-ddfd-4c77-e98f-95b248dd3392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 [==============================] - 211s 269ms/step - loss: 5.6918 - accuracy: 0.1156\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 216s 277ms/step - loss: 5.0472 - accuracy: 0.1880\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 207s 264ms/step - loss: 4.7785 - accuracy: 0.2129\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 216s 276ms/step - loss: 4.6381 - accuracy: 0.2209\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 207s 265ms/step - loss: 4.4936 - accuracy: 0.2288\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 229s 293ms/step - loss: 4.3731 - accuracy: 0.2358\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 214s 274ms/step - loss: 4.2653 - accuracy: 0.2412\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 211s 270ms/step - loss: 4.1586 - accuracy: 0.2477\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 208s 266ms/step - loss: 4.0597 - accuracy: 0.2541\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 213s 273ms/step - loss: 3.9709 - accuracy: 0.2600\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 211s 270ms/step - loss: 3.8940 - accuracy: 0.2656\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 210s 269ms/step - loss: 3.8243 - accuracy: 0.2713\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 217s 277ms/step - loss: 3.7596 - accuracy: 0.2764\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 216s 277ms/step - loss: 3.7011 - accuracy: 0.2818\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 222s 284ms/step - loss: 3.6454 - accuracy: 0.2852\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 230s 295ms/step - loss: 3.5923 - accuracy: 0.2900\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 213s 272ms/step - loss: 3.5413 - accuracy: 0.2945\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 221s 283ms/step - loss: 3.4960 - accuracy: 0.2990\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 215s 275ms/step - loss: 3.4497 - accuracy: 0.3037\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 206s 263ms/step - loss: 3.4073 - accuracy: 0.3074\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 219s 280ms/step - loss: 3.3670 - accuracy: 0.3118\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 215s 274ms/step - loss: 3.3282 - accuracy: 0.3162\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 221s 282ms/step - loss: 3.2900 - accuracy: 0.3198\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 206s 264ms/step - loss: 3.2548 - accuracy: 0.3239\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 213s 273ms/step - loss: 3.2207 - accuracy: 0.3285\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 214s 273ms/step - loss: 3.1872 - accuracy: 0.3324\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 227s 290ms/step - loss: 3.1567 - accuracy: 0.3354\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 216s 276ms/step - loss: 3.1275 - accuracy: 0.3385\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 206s 264ms/step - loss: 3.0973 - accuracy: 0.3429\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 222s 284ms/step - loss: 3.0704 - accuracy: 0.3460\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 228s 291ms/step - loss: 3.0440 - accuracy: 0.3494\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 223s 285ms/step - loss: 3.0180 - accuracy: 0.3524\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 225s 287ms/step - loss: 2.9922 - accuracy: 0.3566\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 221s 283ms/step - loss: 2.9676 - accuracy: 0.3592\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 211s 270ms/step - loss: 2.9452 - accuracy: 0.3627\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 218s 279ms/step - loss: 2.9216 - accuracy: 0.3660\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 214s 274ms/step - loss: 2.8987 - accuracy: 0.3684\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 211s 270ms/step - loss: 2.8778 - accuracy: 0.3719\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 227s 290ms/step - loss: 2.8559 - accuracy: 0.3742\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 224s 286ms/step - loss: 2.8340 - accuracy: 0.3778\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 226s 289ms/step - loss: 2.8149 - accuracy: 0.3801\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 226s 290ms/step - loss: 2.7942 - accuracy: 0.3838\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 218s 279ms/step - loss: 2.7747 - accuracy: 0.3865\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 218s 279ms/step - loss: 2.7553 - accuracy: 0.3889\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 221s 282ms/step - loss: 2.7369 - accuracy: 0.3922\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 219s 280ms/step - loss: 2.7186 - accuracy: 0.3944\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 228s 291ms/step - loss: 2.6996 - accuracy: 0.3974\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 228s 291ms/step - loss: 2.6913 - accuracy: 0.3989\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 230s 294ms/step - loss: 2.6732 - accuracy: 0.4007\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 238s 305ms/step - loss: 2.6505 - accuracy: 0.4056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe54ceba630>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size = 256, epochs = 50) #fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eRMOK1UMRIM"
   },
   "source": [
    "# TEXT GENERATION\n",
    "\n",
    "once the model is trained, we can proceed with the main goal of this notebook: The continous generation of text. For this, we need to take into account that the model receives an $N$ word set as a **seed** for generationg the next word. Given this, it is important that this seed is coherent wih the text structure given as a training set, since the model learned not just to recognize words and to predict them, but to verify the order of the seed words in order to give a reasonaable output as an answer. \n",
    "\n",
    "\n",
    "`generate_text_seq()` generates `n_words` number of words after the given `seed_text`. We are going to pre-process the seed_text before predicting. We are going to encode the seed_text using the same encoding used for encoding the training data. Then we are going to convert the seed_text to $N$ words by using `pad_sequences()`. Now we will predict using `model.predict_classes()`. After that we will search the word in tokenizer using the index in `y_predict` (the output vector). Finally we will append the predicted word to seed_text and text and repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYEI3DuK0Byw"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This method receives:\n",
    " * model: the trained model\n",
    " * tokenizer: the fitted dictionary\n",
    " * text_seq_length: the N length of each word sequence\n",
    " * seed_text: the words that will be used as seed for text generation\n",
    " * n_words: the number of word to be generated\n",
    "'''\n",
    "\n",
    "\n",
    "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n",
    "  \n",
    "    text = []\n",
    "\n",
    "    for _ in range(n_words):\n",
    "        # translate the seed text\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        #make the sequences of length N (by padding or truncating)\n",
    "        encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre') \n",
    "\n",
    "        # generate the response vector using the trained model\n",
    "        y_predict = model.predict_classes(encoded) \n",
    "\n",
    "        #get the predicted word\n",
    "        predicted_word = '' \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "          #find it in dictionary\n",
    "          if index == y_predict:\n",
    "            predicted_word = word \n",
    "            break\n",
    "\n",
    "    # append the new word to the seed_text for the next word\n",
    "    seed_text = seed_text + ' ' + predicted_word\n",
    "    #append the new word to the list of words\n",
    "    text.append(predicted_word)\n",
    "\n",
    "    #return a jointed list of words\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gESXIAkYPmwf"
   },
   "source": [
    "### Checking results\n",
    "\n",
    "Now, lets generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "eg0JFybr0kNO",
    "outputId": "196222e2-3dea-4e43-d9b2-d987b4c1ecba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  199951  lines\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'founder who made thereof a graven image and a molten image and they were in the house of micah and the man micah had an house of gods and made an ephod and teraphim and consecrated one of his sons who became his priest in those days there was no king'"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_lines  = len(lines)\n",
    "print(\"There are \",n_lines,\" lines\")\n",
    "seed_text = lines[190000] # use arbitrary line as seed\n",
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xg3MwYCeP61V",
    "outputId": "be73e6b6-a11f-4785-e943-d7175bf80716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "founder who made thereof a graven image and a molten image and they were in the house of micah and the man micah had an house of gods and made an ephod and teraphim and consecrated one of his sons who became his priest in those days there was no king of slaying him and slew of the pursuers and the lord said unto moses stretch out of the land of egypt and the lord said unto moses stretch out of the land of egypt and the lord spake unto moses saying speak unto the children of israel and say unto them this day and the lord spake unto moses saying speak unto the children of israel and say unto them this day and the lord spake unto moses saying speak unto the children of israel and say unto them this day and the lord said unto moses take yourselves in\n"
     ]
    }
   ],
   "source": [
    "#generate text (100 words)\n",
    "text_gen = generate_text_seq(loaded_model, tokenizer, seq_length, seed_text, 100)\n",
    "\n",
    "print(seed_text + ' ' + text_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3NnZXhePGio"
   },
   "source": [
    "# Extra assets\n",
    "\n",
    "This section might be useful to test results or check some settings in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cu59__QtSsJO"
   },
   "source": [
    "## Using TPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUbQQDin5nsj",
    "outputId": "6d10dcef-a7e8-4353-c73b-c34ce9ff3df9"
   },
   "outputs": [],
   "source": [
    "# IS TPU GOING TO BE USED?\n",
    "## THIS MIGHT NOT BE AVAILABLE FOR LOCAL EXECUTION (TPU AVILABLE IN GOOGLE COLAB)\n",
    "\n",
    "call_TPU = True\n",
    "if(call_TPU):\n",
    "    %tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2abK7KoWR42O"
   },
   "source": [
    "## Saving model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aENl7NkJNB7y",
    "outputId": "2b0d0a79-8de7-4844-d251-4edfddc61f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(drivepath+\"model_textgen_v3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(drivepath+\"model_textgen_v3_weights.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6pCLQ3PR-jr"
   },
   "source": [
    "## Loading model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWk0wTUZSZ8v",
    "outputId": "8306de74-96df-4bc5-b565-b8dec4ccfea0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open(drivepath+\"model_textgen_v3.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(drivepath+\"model_textgen_v3_weights.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TextGen_V3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
